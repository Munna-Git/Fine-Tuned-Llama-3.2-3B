

## ğŸš€ **Challenges Overcome During Fine-Tuning a Legal Domain Llama 3.2 Model**

### **1ï¸âƒ£ Hardware & Environment Challenges**

* âš™ï¸ **Limited GPU Memory (T4x2)** â€”
  Fine-tuning a 3B parameter model required optimization to fit within Kaggleâ€™s 32 GB GPU limit.
  âœ… *Solution:* Used **gradient checkpointing**, **8-bit quantization**, and **LoRA adapters** to drastically reduce VRAM usage.

* ğŸ§© **Session Timeouts in Kaggle Environment** â€”
  Kaggle kernels have strict runtime limits that often interrupted long training jobs.
  âœ… *Solution:* Broke training into multiple runs and managed checkpoint saving/loading efficiently to resume from the last saved step.

* âš¡ **Dependency & Library Conflicts** â€”
  Installing the correct versions of `transformers`, `accelerate`, `peft`, and `bitsandbytes` for Llama 3.2 compatibility required several environment cleanups.
  âœ… *Solution:* Created a **clean environment cell** that uninstalled conflicting versions and reinstalled the exact compatible libraries.

---

### **2ï¸âƒ£ Model Integration Challenges**

* ğŸ§  **LoRA Adapter Handling** â€”
  Understanding where the *fine-tuned weights actually reside* (in LoRA adapters vs. full checkpoints) caused confusion initially.
  âœ… *Solution:* Identified that **`lora_adapters/`** contained only delta weights and **`checkpoint-297/`** was the final fine-tuned model; documented it clearly for reproducibility.

* ğŸª„ **Decoding Output Errors** â€”
  Encountered errors like:

  ```
  argument 'ids': 'list' object cannot be interpreted as an integer
  ```

  due to passing incorrect tensor shapes to the tokenizer.
  âœ… *Solution:* Diagnosed the cause and fixed the decoding logic by using `outputs[0]` and safe string operations (`split()[0].strip()`).

* ğŸ”„ **Merging LoRA with Base Model** â€”
  The model outputs were confusing when only adapters were loaded.
  âœ… *Solution:* Learned to **merge LoRA adapters with the base Llama model** for consistent inference and export to Hugging Face.

---

### **3ï¸âƒ£ Code & Inference Challenges**

* ğŸ§¾ **Prompt Formatting for Llama 3.2 Chat Template** â€”
  Using `<|begin_of_text|>` and `<|start_header_id|>` tokens correctly was crucial for getting structured assistant outputs.
  âœ… *Solution:* Followed the Llama 3.2 chat formatting documentation and created consistent prompt templates for clause extraction.

* ğŸ’¬ **Output Parsing Issues** â€”
  The model sometimes returned full context instead of just the clause.
  âœ… *Solution:* Built a robust text extraction and cleanup pipeline using `split()` and filtering to isolate assistant responses.

* ğŸ• **Performance & Latency Tracking** â€”
  Needed to measure how fast the model generated clauses on Kaggleâ€™s limited compute.
  âœ… *Solution:* Added precise latency measurement (`time.time()`) and confidence estimation logic to monitor inference performance.

---

### **4ï¸âƒ£ Deployment & Access Challenges**

* ğŸŒ **Downloading Fine-Tuned Model from Kaggle** â€”
  Kaggle stores model checkpoints in its ephemeral session storage, making retrieval tricky.
  âœ… *Solution:* Used the **Kaggle CLI (`kaggle kernels output ...`)** and later **Hugging Face CLI** to export and persist the model safely.

* ğŸ§© **Hugging Face CLI Setup on Windows** â€”
  Faced multiple `â€˜huggingface-cliâ€™ not recognized` and `ModuleNotFoundError` issues due to environment paths.
  âœ… *Solution:* Installed `huggingface_hub` correctly, used `python -m huggingface_hub.cli` as a reliable workaround, and verified the repo IDs manually.

---

### **5ï¸âƒ£ Learning & Optimization Takeaways**

* ğŸ’¡ Learned to manage **large-model fine-tuning efficiently on limited resources**.
* ğŸ’¡ Developed an **error-handling wrapper** (`tryâ€“except`) to capture and return detailed inference errors gracefully.
* ğŸ’¡ Built a deeper understanding of **transformersâ€™ tokenization pipeline**, model generation logic, and **LoRA-based fine-tuning**.
* ğŸ’¡ Gained hands-on experience in **exporting models to Hugging Face Hub**, ensuring public accessibility and version tracking.

---

### âœ… **Summary (What It Proves to Recruiters or Managers)**

> You didnâ€™t just run a notebook â€” you **engineered a solution**.
> You handled hardware limits, debugging, model architecture understanding, prompt design, and deployment â€” which mirrors **real-world ML workflow challenges** in production teams.

---

